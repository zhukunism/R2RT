{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(num_epochs, num_steps):\n",
    "    for i in range(num_epochs):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L95\n",
    "\"\"\"\n",
    "\n",
    "def rnn_cell(rnn_input, state, state_size, num_classes):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    print(\"rnn_cell: W.name=%s b.name=%s\" % (W.name,b.name))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(rnn_outputs, state_size, num_classes, useDynamic=False):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    #logits and predictions\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    if useDynamic:\n",
    "        print(\"rnn_outputs\",type(rnn_outputs),rnn_outputs.shape)\n",
    "        logits = tf.reshape(tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) + b, \\\n",
    "            [batch_size, num_steps, num_classes])\n",
    "    else:\n",
    "        print(\"rnn_outputs\",type(rnn_outputs),len(rnn_outputs))\n",
    "        logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "    print(\"output: W.name=%s b.name=%s\" % (W.name,b.name))\n",
    "    # predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cnn_net(x, batch_size, state_size, num_classes, useTF=False, useDynamic=False):\n",
    "    if useDynamic:\n",
    "        assert useTF == True\n",
    "    \n",
    "    init_state = tf.placeholder(tf.float32, shape=(batch_size, state_size), name='init_state')\n",
    "\n",
    "    \"\"\"\n",
    "    RNN Inputs\n",
    "    \"\"\"\n",
    "    # Turn our x placeholder into a list of one-hot tensors:\n",
    "    # rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "    x_one_hot = tf.one_hot(x, num_classes)\n",
    "    if useDynamic:\n",
    "        rnn_inputs = x_one_hot\n",
    "        print(\"rnn_inputs\",type(rnn_inputs),rnn_inputs.shape)\n",
    "    else:\n",
    "        rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "        print(\"rnn_inputs\",type(rnn_inputs),len(rnn_inputs))\n",
    "\n",
    "    \"\"\"\n",
    "    Adding rnn_cells to graph\n",
    "\n",
    "    This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "    Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "    \"\"\"\n",
    "    if useTF:\n",
    "        cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "        print(\"BasicRNNCell: \",type(cell))\n",
    "        if useDynamic:\n",
    "            rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "        else:\n",
    "            rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "    else:\n",
    "        # create cnn variables to reuse\n",
    "        with tf.variable_scope('rnn_cell'):\n",
    "            W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "            b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        state = init_state\n",
    "        rnn_outputs = []\n",
    "        for rnn_input in rnn_inputs:\n",
    "            state = rnn_cell(rnn_input, state, state_size, num_classes)\n",
    "            rnn_outputs.append(state)\n",
    "        final_state = rnn_outputs[-1]\n",
    "\n",
    "        print(\"state\", type(state), state.shape)\n",
    "        \n",
    "    print(\"init_state\", type(init_state), init_state.shape)\n",
    "    print(\"final_state\", type(final_state), final_state.shape)\n",
    "        \n",
    "    logits = output(rnn_outputs, state_size, num_classes, useDynamic)\n",
    "    \n",
    "    if useDynamic:\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    else:\n",
    "        # Turn our y placeholder into a list of labels\n",
    "        y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "        print(\"y_as_list\",type(y_as_list),len(y_as_list))\n",
    "\n",
    "        #losses and train_step\n",
    "        losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "                  logit, label in zip(logits, y_as_list)]\n",
    "\n",
    "    return [final_state, logits, losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(final_state, logits, losses, total_loss, num_epochs, num_steps, state_size, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={'x:0':X, 'y:0':Y, 'init_state:0':training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x <class 'tensorflow.python.framework.ops.Tensor'> (200, 10)\n",
      "y <class 'tensorflow.python.framework.ops.Tensor'> (200, 10)\n",
      "rnn_inputs <class 'tensorflow.python.framework.ops.Tensor'> (200, 10, 2)\n",
      "BasicRNNCell:  <class 'tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicRNNCell'>\n",
      "init_state <class 'tensorflow.python.framework.ops.Tensor'> (200, 16)\n",
      "final_state <class 'tensorflow.python.framework.ops.Tensor'> (200, 16)\n",
      "rnn_outputs <class 'tensorflow.python.framework.ops.Tensor'> (200, 10, 16)\n",
      "output: W.name=softmax/W:0 b.name=softmax/b:0\n"
     ]
    }
   ],
   "source": [
    "# Global config variables\n",
    "num_steps = 10 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 16\n",
    "learning_rate = 0.1\n",
    "useTF = True\n",
    "useDynamic = True\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')\n",
    "print(\"x\",type(x),x.shape)\n",
    "print(\"y\",type(y),y.shape)\n",
    "\n",
    "final_state, logits, losses = cnn_net(x, batch_size, state_size, num_classes, useTF=useTF, useDynamic=useDynamic)\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 250 steps: 0.516372788846\n",
      "Average loss at step 200 for last 250 steps: 0.482183126211\n",
      "Average loss at step 300 for last 250 steps: 0.478338264227\n",
      "Average loss at step 400 for last 250 steps: 0.472839602232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xd5b7278>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEmFU/CIiEUbFLyISYVT8IiIR5n8Bp8qNv6uXWsQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x459080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "training_losses = train_network(final_state, logits, losses, total_loss, num_epochs,num_steps,state_size)\n",
    "plt.plot(training_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
